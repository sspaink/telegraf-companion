[
	{
		"Name": "amon",
		"Description": " Configuration for Amon Server to send metrics to.",
		"SampleConfig": "[[outputs.amon]]\n  ## Amon Server Key\n  server_key = \"my-server-key\" # required.\n\n  ## Amon Instance URL\n  amon_instance = \"https://youramoninstance\" # required\n\n  ## Connection timeout.\n  # timeout = \"5s\"\n"
	},
	{
		"Name": "amqp",
		"Description": " Publishes metrics to an AMQP broker",
		"SampleConfig": "[[outputs.amqp]]\n  ## Broker to publish to.\n  ##   deprecated in 1.7; use the brokers option\n  # url = \"amqp://localhost:5672/influxdb\"\n\n  ## Brokers to publish to.  If multiple brokers are specified a random broker\n  ## will be selected anytime a connection is established.  This can be\n  ## helpful for load balancing when not using a dedicated load balancer.\n  brokers = [\"amqp://localhost:5672/influxdb\"]\n\n  ## Maximum messages to send over a connection.  Once this is reached, the\n  ## connection is closed and a new connection is made.  This can be helpful for\n  ## load balancing when not using a dedicated load balancer.\n  # max_messages = 0\n\n  ## Exchange to declare and publish to.\n  exchange = \"telegraf\"\n\n  ## Exchange type; common types are \"direct\", \"fanout\", \"topic\", \"header\", \"x-consistent-hash\".\n  # exchange_type = \"topic\"\n\n  ## If true, exchange will be passively declared.\n  # exchange_passive = false\n\n  ## Exchange durability can be either \"transient\" or \"durable\".\n  # exchange_durability = \"durable\"\n\n  ## Additional exchange arguments.\n  # exchange_arguments = { }\n  # exchange_arguments = {\"hash_property\" = \"timestamp\"}\n\n  ## Authentication credentials for the PLAIN auth_method.\n  # username = \"\"\n  # password = \"\"\n\n  ## Auth method. PLAIN and EXTERNAL are supported\n  ## Using EXTERNAL requires enabling the rabbitmq_auth_mechanism_ssl plugin as\n  ## described here: https://www.rabbitmq.com/plugins.html\n  # auth_method = \"PLAIN\"\n\n  ## Metric tag to use as a routing key.\n  ##   ie, if this tag exists, its value will be used as the routing key\n  # routing_tag = \"host\"\n\n  ## Static routing key.  Used when no routing_tag is set or as a fallback\n  ## when the tag specified in routing tag is not found.\n  # routing_key = \"\"\n  # routing_key = \"telegraf\"\n\n  ## Delivery Mode controls if a published message is persistent.\n  ##   One of \"transient\" or \"persistent\".\n  # delivery_mode = \"transient\"\n\n  ## InfluxDB database added as a message header.\n  ##   deprecated in 1.7; use the headers option\n  # database = \"telegraf\"\n\n  ## InfluxDB retention policy added as a message header\n  ##   deprecated in 1.7; use the headers option\n  # retention_policy = \"default\"\n\n  ## Static headers added to each published message.\n  # headers = { }\n  # headers = {\"database\" = \"telegraf\", \"retention_policy\" = \"default\"}\n\n  ## Connection timeout.  If not provided, will default to 5s.  0s means no\n  ## timeout (not recommended).\n  # timeout = \"5s\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n\n  ## If true use batch serialization format instead of line based delimiting.\n  ## Only applies to data formats which are not line based such as JSON.\n  ## Recommended to set to true.\n  # use_batch_format = false\n\n  ## Content encoding for message payloads, can be set to \"gzip\" to or\n  ## \"identity\" to apply no encoding.\n  ##\n  ## Please note that when use_batch_format = false each amqp message contains only\n  ## a single metric, it is recommended to use compression with batch format\n  ## for best results.\n  # content_encoding = \"identity\"\n\n  ## Data format to output.\n  ## Each data format has its own unique set of configuration options, read\n  ## more about them here:\n  ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md\n  # data_format = \"influx\"\n"
	},
	{
		"Name": "application_insights",
		"Description": " Send metrics to Azure Application Insights",
		"SampleConfig": "[[outputs.application_insights]]\n  ## Instrumentation key of the Application Insights resource.\n  instrumentation_key = \"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxx\"\n\n  ## Regions that require endpoint modification https://docs.microsoft.com/en-us/azure/azure-monitor/app/custom-endpoints\n  # endpoint_url = \"https://dc.services.visualstudio.com/v2/track\"\n\n  ## Timeout for closing (default: 5s).\n  # timeout = \"5s\"\n\n  ## Enable additional diagnostic logging.\n  # enable_diagnostic_logging = false\n\n  ## Context Tag Sources add Application Insights context tags to a tag value.\n  ##\n  ## For list of allowed context tag keys see:\n  ## https://github.com/microsoft/ApplicationInsights-Go/blob/master/appinsights/contracts/contexttagkeys.go\n  # [outputs.application_insights.context_tag_sources]\n  #   \"ai.cloud.role\" = \"kubernetes_container_name\"\n  #   \"ai.cloud.roleInstance\" = \"kubernetes_pod_name\"\n"
	},
	{
		"Name": "azure_data_explorer",
		"Description": " Sends metrics to Azure Data Explorer",
		"SampleConfig": "[[outputs.azure_data_explorer]]\n  ## The URI property of the Azure Data Explorer resource on Azure\n  ## ex: endpoint_url = https://myadxresource.australiasoutheast.kusto.windows.net\n  endpoint_url = \"\"\n\n  ## The Azure Data Explorer database that the metrics will be ingested into.\n  ## The plugin will NOT generate this database automatically, it's expected that this database already exists before ingestion.\n  ## ex: \"exampledatabase\"\n  database = \"\"\n\n  ## Timeout for Azure Data Explorer operations\n  # timeout = \"20s\"\n\n  ## Type of metrics grouping used when pushing to Azure Data Explorer.\n  ## Default is \"TablePerMetric\" for one table per different metric.\n  ## For more information, please check the plugin README.\n  # metrics_grouping_type = \"TablePerMetric\"\n\n  ## Name of the single table to store all the metrics (Only needed if metrics_grouping_type is \"SingleTable\").\n  # table_name = \"\"\n\n  ## Creates tables and relevant mapping if set to true(default).\n  ## Skips table and mapping creation if set to false, this is useful for running Telegraf with the lowest possible permissions i.e. table ingestor role.\n  # create_tables = true\n"
	},
	{
		"Name": "azure_monitor",
		"Description": " Send aggregate metrics to Azure Monitor",
		"SampleConfig": "[[outputs.azure_monitor]]\n  ## Timeout for HTTP writes.\n  # timeout = \"20s\"\n\n  ## Set the namespace prefix, defaults to \"Telegraf/\u003cinput-name\u003e\".\n  # namespace_prefix = \"Telegraf/\"\n\n  ## Azure Monitor doesn't have a string value type, so convert string\n  ## fields to dimensions (a.k.a. tags) if enabled. Azure Monitor allows\n  ## a maximum of 10 dimensions so Telegraf will only send the first 10\n  ## alphanumeric dimensions.\n  # strings_as_dimensions = false\n\n  ## Both region and resource_id must be set or be available via the\n  ## Instance Metadata service on Azure Virtual Machines.\n  #\n  ## Azure Region to publish metrics against.\n  ##   ex: region = \"southcentralus\"\n  # region = \"\"\n  #\n  ## The Azure Resource ID against which metric will be logged, e.g.\n  ##   ex: resource_id = \"/subscriptions/\u003csubscription_id\u003e/resourceGroups/\u003cresource_group\u003e/providers/Microsoft.Compute/virtualMachines/\u003cvm_name\u003e\"\n  # resource_id = \"\"\n\n  ## Optionally, if in Azure US Government, China, or other sovereign\n  ## cloud environment, set the appropriate REST endpoint for receiving\n  ## metrics. (Note: region may be unused in this context)\n  # endpoint_url = \"https://monitoring.core.usgovcloudapi.net\"\n"
	},
	{
		"Name": "bigquery",
		"Description": " Configuration for Google Cloud BigQuery to send entries",
		"SampleConfig": "[[outputs.bigquery]]\n  ## Credentials File\n  credentials_file = \"/path/to/service/account/key.json\"\n\n  ## Google Cloud Platform Project\n  project = \"my-gcp-project\"\n\n  ## The namespace for the metric descriptor\n  dataset = \"telegraf\"\n\n  ## Timeout for BigQuery operations.\n  # timeout = \"5s\"\n\n  ## Character to replace hyphens on Metric name\n  # replace_hyphen_to = \"_\"\n"
	},
	{
		"Name": "cloud_pubsub",
		"Description": " Publish Telegraf metrics to a Google Cloud PubSub topic",
		"SampleConfig": "[[outputs.cloud_pubsub]]\n  ## Required. Name of Google Cloud Platform (GCP) Project that owns\n  ## the given PubSub topic.\n  project = \"my-project\"\n\n  ## Required. Name of PubSub topic to publish metrics to.\n  topic = \"my-topic\"\n\n  ## Required. Data format to consume.\n  ## Each data format has its own unique set of configuration options.\n  ## Read more about them here:\n  ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md\n  data_format = \"influx\"\n\n  ## Optional. Filepath for GCP credentials JSON file to authorize calls to\n  ## PubSub APIs. If not set explicitly, Telegraf will attempt to use\n  ## Application Default Credentials, which is preferred.\n  # credentials_file = \"path/to/my/creds.json\"\n\n  ## Optional. If true, will send all metrics per write in one PubSub message.\n  # send_batched = true\n\n  ## The following publish_* parameters specifically configures batching\n  ## requests made to the GCP Cloud PubSub API via the PubSub Golang library. Read\n  ## more here: https://godoc.org/cloud.google.com/go/pubsub#PublishSettings\n\n  ## Optional. Send a request to PubSub (i.e. actually publish a batch)\n  ## when it has this many PubSub messages. If send_batched is true,\n  ## this is ignored and treated as if it were 1.\n  # publish_count_threshold = 1000\n\n  ## Optional. Send a request to PubSub (i.e. actually publish a batch)\n  ## when it has this many PubSub messages. If send_batched is true,\n  ## this is ignored and treated as if it were 1\n  # publish_byte_threshold = 1000000\n\n  ## Optional. Specifically configures requests made to the PubSub API.\n  # publish_num_go_routines = 2\n\n  ## Optional. Specifies a timeout for requests to the PubSub API.\n  # publish_timeout = \"30s\"\n\n  ## Optional. If true, published PubSub message data will be base64-encoded.\n  # base64_data = false\n\n  ## Optional. PubSub attributes to add to metrics.\n  # [outputs.cloud_pubsub.attributes]\n  #   my_attr = \"tag_value\"\n"
	},
	{
		"Name": "cloudwatch",
		"Description": " Configuration for AWS CloudWatch output.",
		"SampleConfig": "[[outputs.cloudwatch]]\n  ## Amazon REGION\n  region = \"us-east-1\"\n\n  ## Amazon Credentials\n  ## Credentials are loaded in the following order\n  ## 1) Web identity provider credentials via STS if role_arn and web_identity_token_file are specified\n  ## 2) Assumed credentials via STS if role_arn is specified\n  ## 3) explicit credentials from 'access_key' and 'secret_key'\n  ## 4) shared profile from 'profile'\n  ## 5) environment variables\n  ## 6) shared credentials file\n  ## 7) EC2 Instance Profile\n  #access_key = \"\"\n  #secret_key = \"\"\n  #token = \"\"\n  #role_arn = \"\"\n  #web_identity_token_file = \"\"\n  #role_session_name = \"\"\n  #profile = \"\"\n  #shared_credential_file = \"\"\n\n  ## Endpoint to make request against, the correct endpoint is automatically\n  ## determined and this option should only be set if you wish to override the\n  ## default.\n  ##   ex: endpoint_url = \"http://localhost:8000\"\n  # endpoint_url = \"\"\n\n  ## Namespace for the CloudWatch MetricDatums\n  namespace = \"InfluxData/Telegraf\"\n\n  ## If you have a large amount of metrics, you should consider to send statistic\n  ## values instead of raw metrics which could not only improve performance but\n  ## also save AWS API cost. If enable this flag, this plugin would parse the required\n  ## CloudWatch statistic fields (count, min, max, and sum) and send them to CloudWatch.\n  ## You could use basicstats aggregator to calculate those fields. If not all statistic\n  ## fields are available, all fields would still be sent as raw metrics.\n  # write_statistics = false\n\n  ## Enable high resolution metrics of 1 second (if not enabled, standard resolution are of 60 seconds precision)\n  # high_resolution_metrics = false\n"
	},
	{
		"Name": "cloudwatch_logs",
		"Description": " Configuration for AWS CloudWatchLogs output.",
		"SampleConfig": "[[outputs.cloudwatch_logs]]\n  ## The region is the Amazon region that you wish to connect to.\n  ## Examples include but are not limited to:\n  ## - us-west-1\n  ## - us-west-2\n  ## - us-east-1\n  ## - ap-southeast-1\n  ## - ap-southeast-2\n  ## ...\n  region = \"us-east-1\"\n\n  ## Amazon Credentials\n  ## Credentials are loaded in the following order\n  ## 1) Web identity provider credentials via STS if role_arn and web_identity_token_file are specified\n  ## 2) Assumed credentials via STS if role_arn is specified\n  ## 3) explicit credentials from 'access_key' and 'secret_key'\n  ## 4) shared profile from 'profile'\n  ## 5) environment variables\n  ## 6) shared credentials file\n  ## 7) EC2 Instance Profile\n  #access_key = \"\"\n  #secret_key = \"\"\n  #token = \"\"\n  #role_arn = \"\"\n  #web_identity_token_file = \"\"\n  #role_session_name = \"\"\n  #profile = \"\"\n  #shared_credential_file = \"\"\n\n  ## Endpoint to make request against, the correct endpoint is automatically\n  ## determined and this option should only be set if you wish to override the\n  ## default.\n  ##   ex: endpoint_url = \"http://localhost:8000\"\n  # endpoint_url = \"\"\n\n  ## Cloud watch log group. Must be created in AWS cloudwatch logs upfront!\n  ## For example, you can specify the name of the k8s cluster here to group logs from all cluster in oine place\n  log_group = \"my-group-name\"\n\n  ## Log stream in log group\n  ## Either log group name or reference to metric attribute, from which it can be parsed:\n  ## tag:\u003cTAG_NAME\u003e or field:\u003cFIELD_NAME\u003e. If log stream is not exist, it will be created.\n  ## Since AWS is not automatically delete logs streams with expired logs entries (i.e. empty log stream)\n  ## you need to put in place appropriate house-keeping (https://forums.aws.amazon.com/thread.jspa?threadID=178855)\n  log_stream = \"tag:location\"\n\n  ## Source of log data - metric name\n  ## specify the name of the metric, from which the log data should be retrieved.\n  ## I.e., if you  are using docker_log plugin to stream logs from container, then\n  ## specify log_data_metric_name  = \"docker_log\"\n  log_data_metric_name  = \"docker_log\"\n\n  ## Specify from which metric attribute the log data should be retrieved:\n  ## tag:\u003cTAG_NAME\u003e or field:\u003cFIELD_NAME\u003e.\n  ## I.e., if you  are using docker_log plugin to stream logs from container, then\n  ## specify log_data_source  = \"field:message\"\n  log_data_source  = \"field:message\"\n"
	},
	{
		"Name": "cratedb",
		"Description": " Configuration for CrateDB to send metrics to.",
		"SampleConfig": "[[outputs.cratedb]]\n  # A github.com/jackc/pgx/v4 connection string.\n  # See https://pkg.go.dev/github.com/jackc/pgx/v4#ParseConfig\n  url = \"postgres://user:password@localhost/schema?sslmode=disable\"\n  # Timeout for all CrateDB queries.\n  timeout = \"5s\"\n  # Name of the table to store metrics in.\n  table = \"metrics\"\n  # If true, and the metrics table does not exist, create it automatically.\n  table_create = true\n  # The character(s) to replace any '.' in an object key with\n  key_separator = \"_\"\n"
	},
	{
		"Name": "datadog",
		"Description": " Configuration for DataDog API to send metrics to.",
		"SampleConfig": "[[outputs.datadog]]\n  ## Datadog API key\n  apikey = \"my-secret-key\"\n\n  ## Connection timeout.\n  # timeout = \"5s\"\n\n  ## Write URL override; useful for debugging.\n  # url = \"https://app.datadoghq.com/api/v1/series\"\n\n  ## Set http_proxy (telegraf uses the system wide proxy settings if it isn't set)\n  # http_proxy_url = \"http://localhost:8888\"\n\n  ## Override the default (none) compression used to send data.\n  ## Supports: \"zlib\", \"none\"\n  # compression = \"none\"\n"
	},
	{
		"Name": "discard",
		"Description": " Send metrics to nowhere at all",
		"SampleConfig": "[[outputs.discard]]\n  # no configuration\n"
	},
	{
		"Name": "dynatrace",
		"Description": " Send telegraf metrics to a Dynatrace environment",
		"SampleConfig": "[[outputs.dynatrace]]\n  ## For usage with the Dynatrace OneAgent you can omit any configuration,\n  ## the only requirement is that the OneAgent is running on the same host.\n  ## Only setup environment url and token if you want to monitor a Host without the OneAgent present.\n  ##\n  ## Your Dynatrace environment URL.\n  ## For Dynatrace OneAgent you can leave this empty or set it to \"http://127.0.0.1:14499/metrics/ingest\" (default)\n  ## For Dynatrace SaaS environments the URL scheme is \"https://{your-environment-id}.live.dynatrace.com/api/v2/metrics/ingest\"\n  ## For Dynatrace Managed environments the URL scheme is \"https://{your-domain}/e/{your-environment-id}/api/v2/metrics/ingest\"\n  url = \"\"\n\n  ## Your Dynatrace API token.\n  ## Create an API token within your Dynatrace environment, by navigating to Settings \u003e Integration \u003e Dynatrace API\n  ## The API token needs data ingest scope permission. When using OneAgent, no API token is required.\n  api_token = \"\"\n\n  ## Optional prefix for metric names (e.g.: \"telegraf\")\n  prefix = \"telegraf\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Optional flag for ignoring tls certificate check\n  # insecure_skip_verify = false\n\n  ## Connection timeout, defaults to \"5s\" if not set.\n  timeout = \"5s\"\n\n  ## If you want metrics to be treated and reported as delta counters, add the metric names here\n  additional_counters = [ ]\n\n  ## Optional dimensions to be added to every metric\n  # [outputs.dynatrace.default_dimensions]\n  # default_key = \"default value\"\n"
	},
	{
		"Name": "elasticsearch",
		"Description": " Configuration for Elasticsearch to send metrics to.",
		"SampleConfig": "[[outputs.elasticsearch]]\n  ## The full HTTP endpoint URL for your Elasticsearch instance\n  ## Multiple urls can be specified as part of the same cluster,\n  ## this means that only ONE of the urls will be written to each interval\n  urls = [ \"http://node1.es.example.com:9200\" ] # required.\n  ## Elasticsearch client timeout, defaults to \"5s\" if not set.\n  timeout = \"5s\"\n  ## Set to true to ask Elasticsearch a list of all cluster nodes,\n  ## thus it is not necessary to list all nodes in the urls config option\n  enable_sniffer = false\n  ## Set to true to enable gzip compression\n  enable_gzip = false\n  ## Set the interval to check if the Elasticsearch nodes are available\n  ## Setting to \"0s\" will disable the health check (not recommended in production)\n  health_check_interval = \"10s\"\n  ## Set the timeout for periodic health checks.\n  # health_check_timeout = \"1s\"\n  ## HTTP basic authentication details.\n  ## HTTP basic authentication details\n  # username = \"telegraf\"\n  # password = \"mypassword\"\n  ## HTTP bearer token authentication details\n  # auth_bearer_token = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9\"\n\n  ## Index Config\n  ## The target index for metrics (Elasticsearch will create if it not exists).\n  ## You can use the date specifiers below to create indexes per time frame.\n  ## The metric timestamp will be used to decide the destination index name\n  # %Y - year (2016)\n  # %y - last two digits of year (00..99)\n  # %m - month (01..12)\n  # %d - day of month (e.g., 01)\n  # %H - hour (00..23)\n  # %V - week of the year (ISO week) (01..53)\n  ## Additionally, you can specify a tag name using the notation {{tag_name}}\n  ## which will be used as part of the index name. If the tag does not exist,\n  ## the default tag value will be used.\n  # index_name = \"telegraf-{{host}}-%Y.%m.%d\"\n  # default_tag_value = \"none\"\n  index_name = \"telegraf-%Y.%m.%d\" # required.\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n\n  ## Template Config\n  ## Set to true if you want telegraf to manage its index template.\n  ## If enabled it will create a recommended index template for telegraf indexes\n  manage_template = true\n  ## The template name used for telegraf indexes\n  template_name = \"telegraf\"\n  ## Set to true if you want telegraf to overwrite an existing template\n  overwrite_template = false\n  ## If set to true a unique ID hash will be sent as sha256(concat(timestamp,measurement,series-hash)) string\n  ## it will enable data resend and update metric points avoiding duplicated metrics with diferent id's\n  force_document_id = false\n\n  ## Specifies the handling of NaN and Inf values.\n  ## This option can have the following values:\n  ##    none    -- do not modify field-values (default); will produce an error if NaNs or infs are encountered\n  ##    drop    -- drop fields containing NaNs or infs\n  ##    replace -- replace with the value in \"float_replacement_value\" (default: 0.0)\n  ##               NaNs and inf will be replaced with the given number, -inf with the negative of that number\n  # float_handling = \"none\"\n  # float_replacement_value = 0.0\n\n  ## Pipeline Config\n  ## To use a ingest pipeline, set this to the name of the pipeline you want to use.\n  # use_pipeline = \"my_pipeline\"\n  ## Additionally, you can specify a tag name using the notation {{tag_name}}\n  ## which will be used as part of the pipeline name. If the tag does not exist,\n  ## the default pipeline will be used as the pipeline. If no default pipeline is set,\n  ## no pipeline is used for the metric.\n  # use_pipeline = \"{{es_pipeline}}\"\n  # default_pipeline = \"my_pipeline\"\n"
	},
	{
		"Name": "event_hubs",
		"Description": " Configuration for Event Hubs output plugin",
		"SampleConfig": "[[outputs.event_hubs]]\n  ## The full connection string to the Event Hub (required)\n  ## The shared access key must have \"Send\" permissions on the target Event Hub.\n  connection_string = \"Endpoint=sb://namespace.servicebus.windows.net/;SharedAccessKeyName=RootManageSharedAccessKey;SharedAccessKey=superSecret1234=;EntityPath=hubName\"\n\n  ## Client timeout (defaults to 30s)\n  # timeout = \"30s\"\n\n  ## Partition key\n  ## Metric tag or field name to use for the event partition key. The value of\n  ## this tag or field is set as the key for events if it exists. If both, tag\n  ## and field, exist the tag is preferred.\n  # partition_key = \"\"\n\n  ## Data format to output.\n  ## Each data format has its own unique set of configuration options, read\n  ## more about them here:\n  ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md\n  data_format = \"json\"\n"
	},
	{
		"Name": "exec",
		"Description": " Send metrics to command as input over stdin",
		"SampleConfig": "[[outputs.exec]]\n  ## Command to ingest metrics via stdin.\n  command = [\"tee\", \"-a\", \"/dev/null\"]\n\n  ## Environment variables\n  ## Array of \"key=value\" pairs to pass as environment variables\n  ## e.g. \"KEY=value\", \"USERNAME=John Doe\",\n  ## \"LD_LIBRARY_PATH=/opt/custom/lib64:/usr/local/libs\"\n  # environment = []\n\n  ## Timeout for command to complete.\n  # timeout = \"5s\"\n\n  ## Data format to output.\n  ## Each data format has its own unique set of configuration options, read\n  ## more about them here:\n  ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md\n  # data_format = \"influx\"\n"
	},
	{
		"Name": "execd",
		"Description": " Run executable as long-running output plugin",
		"SampleConfig": "[[outputs.execd]]\n  ## One program to run as daemon.\n  ## NOTE: process and each argument should each be their own string\n  command = [\"my-telegraf-output\", \"--some-flag\", \"value\"]\n\n  ## Environment variables\n  ## Array of \"key=value\" pairs to pass as environment variables\n  ## e.g. \"KEY=value\", \"USERNAME=John Doe\",\n  ## \"LD_LIBRARY_PATH=/opt/custom/lib64:/usr/local/libs\"\n  # environment = []\n\n  ## Delay before the process is restarted after an unexpected termination\n  restart_delay = \"10s\"\n\n  ## Data format to export.\n  ## Each data format has its own unique set of configuration options, read\n  ## more about them here:\n  ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md\n  data_format = \"influx\"\n"
	},
	{
		"Name": "file",
		"Description": " Send telegraf metrics to file(s)",
		"SampleConfig": "[[outputs.file]]\n  ## Files to write to, \"stdout\" is a specially handled file.\n  files = [\"stdout\", \"/tmp/metrics.out\"]\n\n  ## Use batch serialization format instead of line based delimiting.  The\n  ## batch format allows for the production of non line based output formats and\n  ## may more efficiently encode and write metrics.\n  # use_batch_format = false\n\n  ## The file will be rotated after the time interval specified.  When set\n  ## to 0 no time based rotation is performed.\n  # rotation_interval = \"0h\"\n\n  ## The logfile will be rotated when it becomes larger than the specified\n  ## size.  When set to 0 no size based rotation is performed.\n  # rotation_max_size = \"0MB\"\n\n  ## Maximum number of rotated archives to keep, any older logs are deleted.\n  ## If set to -1, no archives are removed.\n  # rotation_max_archives = 5\n\n  ## Data format to output.\n  ## Each data format has its own unique set of configuration options, read\n  ## more about them here:\n  ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md\n  data_format = \"influx\"\n"
	},
	{
		"Name": "graphite",
		"Description": " Configuration for Graphite server to send metrics to",
		"SampleConfig": "[[outputs.graphite]]\n  ## TCP endpoint for your graphite instance.\n  ## If multiple endpoints are configured, the output will be load balanced.\n  ## Only one of the endpoints will be written to with each iteration.\n  servers = [\"localhost:2003\"]\n  ## Prefix metrics name\n  prefix = \"\"\n  ## Graphite output template\n  ## see https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md\n  template = \"host.tags.measurement.field\"\n\n  ## Enable Graphite tags support\n  # graphite_tag_support = false\n\n  ## Define how metric names and tags are sanitized; options are \"strict\", or \"compatible\"\n  ## strict - Default method, and backwards compatible with previous versionf of Telegraf\n  ## compatible - More relaxed sanitizing when using tags, and compatible with the graphite spec\n  # graphite_tag_sanitize_mode = \"strict\"\n\n  ## Character for separating metric name and field for Graphite tags\n  # graphite_separator = \".\"\n\n  ## Graphite templates patterns\n  ## 1. Template for cpu\n  ## 2. Template for disk*\n  ## 3. Default template\n  # templates = [\n  #  \"cpu tags.measurement.host.field\",\n  #  \"disk* measurement.field\",\n  #  \"host.measurement.tags.field\"\n  #]\n\n  ## timeout in seconds for the write connection to graphite\n  timeout = 2\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n"
	},
	{
		"Name": "graylog",
		"Description": " Send telegraf metrics to graylog",
		"SampleConfig": "[[outputs.graylog]]\n  ## Endpoints for your graylog instances.\n  servers = [\"udp://127.0.0.1:12201\"]\n\n  ## Connection timeout.\n  # timeout = \"5s\"\n\n  ## The field to use as the GELF short_message, if unset the static string\n  ## \"telegraf\" will be used.\n  ##   example: short_message_field = \"message\"\n  # short_message_field = \"\"\n\n  ## According to GELF payload specification, additional fields names must be prefixed\n  ## with an underscore. Previous versions did not prefix custom field 'name' with underscore.\n  ## Set to true for backward compatibility.\n  # name_field_no_prefix = false\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n"
	},
	{
		"Name": "groundwork",
		"Description": " Send telegraf metrics to GroundWork Monitor",
		"SampleConfig": "[[outputs.groundwork]]\n  ## URL of your groundwork instance.\n  url = \"https://groundwork.example.com\"\n\n  ## Agent uuid for GroundWork API Server.\n  agent_id = \"\"\n\n  ## Username and password to access GroundWork API.\n  username = \"\"\n  password = \"\"\n\n  ## Default display name for the host with services(metrics).\n  # default_host = \"telegraf\"\n\n  ## Default service state.\n  # default_service_state = \"SERVICE_OK\"\n\n  ## The name of the tag that contains the hostname.\n  # resource_tag = \"host\"\n\n  ## The name of the tag that contains the host group name.\n  # group_tag = \"group\"\n"
	},
	{
		"Name": "health",
		"Description": " Configurable HTTP health check resource based on metrics",
		"SampleConfig": "[[outputs.health]]\n  ## Address and port to listen on.\n  ##   ex: service_address = \"http://localhost:8080\"\n  ##       service_address = \"unix:///var/run/telegraf-health.sock\"\n  # service_address = \"http://:8080\"\n\n  ## The maximum duration for reading the entire request.\n  # read_timeout = \"5s\"\n  ## The maximum duration for writing the entire response.\n  # write_timeout = \"5s\"\n\n  ## Username and password to accept for HTTP basic authentication.\n  # basic_username = \"user1\"\n  # basic_password = \"secret\"\n\n  ## Allowed CA certificates for client certificates.\n  # tls_allowed_cacerts = [\"/etc/telegraf/clientca.pem\"]\n\n  ## TLS server certificate and private key.\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n\n  ## One or more check sub-tables should be defined, it is also recommended to\n  ## use metric filtering to limit the metrics that flow into this output.\n  ##\n  ## When using the default buffer sizes, this example will fail when the\n  ## metric buffer is half full.\n  ##\n  ## namepass = [\"internal_write\"]\n  ## tagpass = { output = [\"influxdb\"] }\n  ##\n  ## [[outputs.health.compares]]\n  ##   field = \"buffer_size\"\n  ##   lt = 5000.0\n  ##\n  ## [[outputs.health.contains]]\n  ##   field = \"buffer_size\"\n"
	},
	{
		"Name": "http",
		"Description": " A plugin that can transmit metrics over HTTP",
		"SampleConfig": "[[outputs.http]]\n  ## URL is the address to send metrics to\n  url = \"http://127.0.0.1:8080/telegraf\"\n\n  ## Timeout for HTTP message\n  # timeout = \"5s\"\n\n  ## HTTP method, one of: \"POST\" or \"PUT\"\n  # method = \"POST\"\n\n  ## HTTP Basic Auth credentials\n  # username = \"username\"\n  # password = \"pa$$word\"\n\n  ## OAuth2 Client Credentials Grant\n  # client_id = \"clientid\"\n  # client_secret = \"secret\"\n  # token_url = \"https://indentityprovider/oauth2/v1/token\"\n  # scopes = [\"urn:opc:idm:__myscopes__\"]\n\n  ## Goole API Auth\n  # google_application_credentials = \"/etc/telegraf/example_secret.json\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n\n  ## Optional Cookie authentication\n  # cookie_auth_url = \"https://localhost/authMe\"\n  # cookie_auth_method = \"POST\"\n  # cookie_auth_username = \"username\"\n  # cookie_auth_password = \"pa$$word\"\n  # cookie_auth_headers = '{\"Content-Type\": \"application/json\", \"X-MY-HEADER\":\"hello\"}'\n  # cookie_auth_body = '{\"username\": \"user\", \"password\": \"pa$$word\", \"authenticate\": \"me\"}'\n  ## cookie_auth_renewal not set or set to \"0\" will auth once and never renew the cookie\n  # cookie_auth_renewal = \"5m\"\n\n  ## Data format to output.\n  ## Each data format has it's own unique set of configuration options, read\n  ## more about them here:\n  ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md\n  # data_format = \"influx\"\n\n  ## Use batch serialization format (default) instead of line based format.\n  ## Batch format is more efficient and should be used unless line based\n  ## format is really needed.\n  # use_batch_format = true\n\n  ## HTTP Content-Encoding for write request body, can be set to \"gzip\" to\n  ## compress body or \"identity\" to apply no encoding.\n  # content_encoding = \"identity\"\n\n  ## Additional HTTP headers\n  # [outputs.http.headers]\n  #   # Should be set manually to \"application/json\" for json data_format\n  #   Content-Type = \"text/plain; charset=utf-8\"\n\n  ## MaxIdleConns controls the maximum number of idle (keep-alive)\n  ## connections across all hosts. Zero means no limit.\n  # max_idle_conn = 0\n\n  ## MaxIdleConnsPerHost, if non-zero, controls the maximum idle\n  ## (keep-alive) connections to keep per-host. If zero,\n  ## DefaultMaxIdleConnsPerHost is used(2).\n  # max_idle_conn_per_host = 2\n\n  ## Idle (keep-alive) connection timeout.\n  ## Maximum amount of time before idle connection is closed.\n  ## Zero means no limit.\n  # idle_conn_timeout = 0\n\n  ## Amazon Region\n  #region = \"us-east-1\"\n\n  ## Amazon Credentials\n  ## Credentials are loaded in the following order\n  ## 1) Web identity provider credentials via STS if role_arn and web_identity_token_file are specified\n  ## 2) Assumed credentials via STS if role_arn is specified\n  ## 3) explicit credentials from 'access_key' and 'secret_key'\n  ## 4) shared profile from 'profile'\n  ## 5) environment variables\n  ## 6) shared credentials file\n  ## 7) EC2 Instance Profile\n  #access_key = \"\"\n  #secret_key = \"\"\n  #token = \"\"\n  #role_arn = \"\"\n  #web_identity_token_file = \"\"\n  #role_session_name = \"\"\n  #profile = \"\"\n  #shared_credential_file = \"\"\n\n  ## Optional list of statuscodes (\u003c200 or \u003e300) upon which requests should not be retried\n  # non_retryable_statuscodes = [409, 413]\n"
	},
	{
		"Name": "influxdb",
		"Description": " Configuration for sending metrics to InfluxDB",
		"SampleConfig": "[[outputs.influxdb]]\n  ## The full HTTP or UDP URL for your InfluxDB instance.\n  ##\n  ## Multiple URLs can be specified for a single cluster, only ONE of the\n  ## urls will be written to each interval.\n  # urls = [\"unix:///var/run/influxdb.sock\"]\n  # urls = [\"udp://127.0.0.1:8089\"]\n  # urls = [\"http://127.0.0.1:8086\"]\n\n  ## The target database for metrics; will be created as needed.\n  ## For UDP url endpoint database needs to be configured on server side.\n  # database = \"telegraf\"\n\n  ## The value of this tag will be used to determine the database.  If this\n  ## tag is not set the 'database' option is used as the default.\n  # database_tag = \"\"\n\n  ## If true, the 'database_tag' will not be included in the written metric.\n  # exclude_database_tag = false\n\n  ## If true, no CREATE DATABASE queries will be sent.  Set to true when using\n  ## Telegraf with a user without permissions to create databases or when the\n  ## database already exists.\n  # skip_database_creation = false\n\n  ## Name of existing retention policy to write to.  Empty string writes to\n  ## the default retention policy.  Only takes effect when using HTTP.\n  # retention_policy = \"\"\n\n  ## The value of this tag will be used to determine the retention policy.  If this\n  ## tag is not set the 'retention_policy' option is used as the default.\n  # retention_policy_tag = \"\"\n\n  ## If true, the 'retention_policy_tag' will not be included in the written metric.\n  # exclude_retention_policy_tag = false\n\n  ## Write consistency (clusters only), can be: \"any\", \"one\", \"quorum\", \"all\".\n  ## Only takes effect when using HTTP.\n  # write_consistency = \"any\"\n\n  ## Timeout for HTTP messages.\n  # timeout = \"5s\"\n\n  ## HTTP Basic Auth\n  # username = \"telegraf\"\n  # password = \"metricsmetricsmetricsmetrics\"\n\n  ## HTTP User-Agent\n  # user_agent = \"telegraf\"\n\n  ## UDP payload size is the maximum packet size to send.\n  # udp_payload = \"512B\"\n\n  ## Optional TLS Config for use on HTTP connections.\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n\n  ## HTTP Proxy override, if unset values the standard proxy environment\n  ## variables are consulted to determine which proxy, if any, should be used.\n  # http_proxy = \"http://corporate.proxy:3128\"\n\n  ## Additional HTTP headers\n  # http_headers = {\"X-Special-Header\" = \"Special-Value\"}\n\n  ## HTTP Content-Encoding for write request body, can be set to \"gzip\" to\n  ## compress body or \"identity\" to apply no encoding.\n  # content_encoding = \"gzip\"\n\n  ## When true, Telegraf will output unsigned integers as unsigned values,\n  ## i.e.: \"42u\".  You will need a version of InfluxDB supporting unsigned\n  ## integer values.  Enabling this option will result in field type errors if\n  ## existing data has been written.\n  # influx_uint_support = false\n"
	},
	{
		"Name": "influxdb_v2",
		"Description": " Configuration for sending metrics to InfluxDB 2.0",
		"SampleConfig": "[[outputs.influxdb_v2]]\n  ## The URLs of the InfluxDB cluster nodes.\n  ##\n  ## Multiple URLs can be specified for a single cluster, only ONE of the\n  ## urls will be written to each interval.\n  ##   ex: urls = [\"https://us-west-2-1.aws.cloud2.influxdata.com\"]\n  urls = [\"http://127.0.0.1:8086\"]\n\n  ## Token for authentication.\n  token = \"\"\n\n  ## Organization is the name of the organization you wish to write to.\n  organization = \"\"\n\n  ## Destination bucket to write into.\n  bucket = \"\"\n\n  ## The value of this tag will be used to determine the bucket.  If this\n  ## tag is not set the 'bucket' option is used as the default.\n  # bucket_tag = \"\"\n\n  ## If true, the bucket tag will not be added to the metric.\n  # exclude_bucket_tag = false\n\n  ## Timeout for HTTP messages.\n  # timeout = \"5s\"\n\n  ## Additional HTTP headers\n  # http_headers = {\"X-Special-Header\" = \"Special-Value\"}\n\n  ## HTTP Proxy override, if unset values the standard proxy environment\n  ## variables are consulted to determine which proxy, if any, should be used.\n  # http_proxy = \"http://corporate.proxy:3128\"\n\n  ## HTTP User-Agent\n  # user_agent = \"telegraf\"\n\n  ## Content-Encoding for write request body, can be set to \"gzip\" to\n  ## compress body or \"identity\" to apply no encoding.\n  # content_encoding = \"gzip\"\n\n  ## Enable or disable uint support for writing uints influxdb 2.0.\n  # influx_uint_support = false\n\n  ## Optional TLS Config for use on HTTP connections.\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n"
	},
	{
		"Name": "instrumental",
		"Description": " Configuration for sending metrics to an Instrumental project",
		"SampleConfig": "[[outputs.instrumental]]\n  ## Project API Token (required)\n  api_token = \"API Token\"  # required\n  ## Prefix the metrics with a given name\n  prefix = \"\"\n  ## Stats output template (Graphite formatting)\n  ## see https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md#graphite\n  template = \"host.tags.measurement.field\"\n  ## Timeout in seconds to connect\n  timeout = \"2s\"\n  ## Debug true - Print communication to Instrumental\n  debug = false\n"
	},
	{
		"Name": "kafka",
		"Description": " Configuration for the Kafka server to send metrics to",
		"SampleConfig": "[[outputs.kafka]]\n  ## URLs of kafka brokers\n  brokers = [\"localhost:9092\"]\n  ## Kafka topic for producer messages\n  topic = \"telegraf\"\n\n  ## The value of this tag will be used as the topic.  If not set the 'topic'\n  ## option is used.\n  # topic_tag = \"\"\n\n  ## If true, the 'topic_tag' will be removed from to the metric.\n  # exclude_topic_tag = false\n\n  ## Optional Client id\n  # client_id = \"Telegraf\"\n\n  ## Set the minimal supported Kafka version.  Setting this enables the use of new\n  ## Kafka features and APIs.  Of particular interested, lz4 compression\n  ## requires at least version 0.10.0.0.\n  ##   ex: version = \"1.1.0\"\n  # version = \"\"\n\n  ## Optional topic suffix configuration.\n  ## If the section is omitted, no suffix is used.\n  ## Following topic suffix methods are supported:\n  ##   measurement - suffix equals to separator + measurement's name\n  ##   tags        - suffix equals to separator + specified tags' values\n  ##                 interleaved with separator\n\n  ## Suffix equals to \"_\" + measurement name\n  # [outputs.kafka.topic_suffix]\n  #   method = \"measurement\"\n  #   separator = \"_\"\n\n  ## Suffix equals to \"__\" + measurement's \"foo\" tag value.\n  ##   If there's no such a tag, suffix equals to an empty string\n  # [outputs.kafka.topic_suffix]\n  #   method = \"tags\"\n  #   keys = [\"foo\"]\n  #   separator = \"__\"\n\n  ## Suffix equals to \"_\" + measurement's \"foo\" and \"bar\"\n  ##   tag values, separated by \"_\". If there is no such tags,\n  ##   their values treated as empty strings.\n  # [outputs.kafka.topic_suffix]\n  #   method = \"tags\"\n  #   keys = [\"foo\", \"bar\"]\n  #   separator = \"_\"\n\n  ## The routing tag specifies a tagkey on the metric whose value is used as\n  ## the message key.  The message key is used to determine which partition to\n  ## send the message to.  This tag is prefered over the routing_key option.\n  routing_tag = \"host\"\n\n  ## The routing key is set as the message key and used to determine which\n  ## partition to send the message to.  This value is only used when no\n  ## routing_tag is set or as a fallback when the tag specified in routing tag\n  ## is not found.\n  ##\n  ## If set to \"random\", a random value will be generated for each message.\n  ##\n  ## When unset, no message key is added and each message is routed to a random\n  ## partition.\n  ##\n  ##   ex: routing_key = \"random\"\n  ##       routing_key = \"telegraf\"\n  # routing_key = \"\"\n\n  ## Compression codec represents the various compression codecs recognized by\n  ## Kafka in messages.\n  ##  0 : None\n  ##  1 : Gzip\n  ##  2 : Snappy\n  ##  3 : LZ4\n  ##  4 : ZSTD\n   # compression_codec = 0\n\n  ## Idempotent Writes\n  ## If enabled, exactly one copy of each message is written.\n  # idempotent_writes = false\n\n  ##  RequiredAcks is used in Produce Requests to tell the broker how many\n  ##  replica acknowledgements it must see before responding\n  ##   0 : the producer never waits for an acknowledgement from the broker.\n  ##       This option provides the lowest latency but the weakest durability\n  ##       guarantees (some data will be lost when a server fails).\n  ##   1 : the producer gets an acknowledgement after the leader replica has\n  ##       received the data. This option provides better durability as the\n  ##       client waits until the server acknowledges the request as successful\n  ##       (only messages that were written to the now-dead leader but not yet\n  ##       replicated will be lost).\n  ##   -1: the producer gets an acknowledgement after all in-sync replicas have\n  ##       received the data. This option provides the best durability, we\n  ##       guarantee that no messages will be lost as long as at least one in\n  ##       sync replica remains.\n  # required_acks = -1\n\n  ## The maximum number of times to retry sending a metric before failing\n  ## until the next flush.\n  # max_retry = 3\n\n  ## The maximum permitted size of a message. Should be set equal to or\n  ## smaller than the broker's 'message.max.bytes'.\n  # max_message_bytes = 1000000\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n\n  ## Optional SOCKS5 proxy to use when connecting to brokers\n  # socks5_enabled = true\n  # socks5_address = \"127.0.0.1:1080\"\n  # socks5_username = \"alice\"\n  # socks5_password = \"pass123\"\n\n  ## Optional SASL Config\n  # sasl_username = \"kafka\"\n  # sasl_password = \"secret\"\n\n  ## Optional SASL:\n  ## one of: OAUTHBEARER, PLAIN, SCRAM-SHA-256, SCRAM-SHA-512, GSSAPI\n  ## (defaults to PLAIN)\n  # sasl_mechanism = \"\"\n\n  ## used if sasl_mechanism is GSSAPI (experimental)\n  # sasl_gssapi_service_name = \"\"\n  # ## One of: KRB5_USER_AUTH and KRB5_KEYTAB_AUTH\n  # sasl_gssapi_auth_type = \"KRB5_USER_AUTH\"\n  # sasl_gssapi_kerberos_config_path = \"/\"\n  # sasl_gssapi_realm = \"realm\"\n  # sasl_gssapi_key_tab_path = \"\"\n  # sasl_gssapi_disable_pafxfast = false\n\n  ## used if sasl_mechanism is OAUTHBEARER (experimental)\n  # sasl_access_token = \"\"\n\n  ## SASL protocol version.  When connecting to Azure EventHub set to 0.\n  # sasl_version = 1\n\n  # Disable Kafka metadata full fetch\n  # metadata_full = false\n\n  ## Data format to output.\n  ## Each data format has its own unique set of configuration options, read\n  ## more about them here:\n  ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md\n  # data_format = \"influx\"\n"
	},
	{
		"Name": "kinesis",
		"Description": " Configuration for the AWS Kinesis output.",
		"SampleConfig": "[[outputs.kinesis]]\n  ## Amazon REGION of kinesis endpoint.\n  region = \"ap-southeast-2\"\n\n  ## Amazon Credentials\n  ## Credentials are loaded in the following order\n  ## 1) Web identity provider credentials via STS if role_arn and web_identity_token_file are specified\n  ## 2) Assumed credentials via STS if role_arn is specified\n  ## 3) explicit credentials from 'access_key' and 'secret_key'\n  ## 4) shared profile from 'profile'\n  ## 5) environment variables\n  ## 6) shared credentials file\n  ## 7) EC2 Instance Profile\n  #access_key = \"\"\n  #secret_key = \"\"\n  #token = \"\"\n  #role_arn = \"\"\n  #web_identity_token_file = \"\"\n  #role_session_name = \"\"\n  #profile = \"\"\n  #shared_credential_file = \"\"\n\n  ## Endpoint to make request against, the correct endpoint is automatically\n  ## determined and this option should only be set if you wish to override the\n  ## default.\n  ##   ex: endpoint_url = \"http://localhost:8000\"\n  # endpoint_url = \"\"\n\n  ## Kinesis StreamName must exist prior to starting telegraf.\n  streamname = \"StreamName\"\n\n  ## The partition key can be calculated using one of several methods:\n  ##\n  ## Use a static value for all writes:\n  #  [outputs.kinesis.partition]\n  #    method = \"static\"\n  #    key = \"howdy\"\n  #\n  ## Use a random partition key on each write:\n  #  [outputs.kinesis.partition]\n  #    method = \"random\"\n  #\n  ## Use the measurement name as the partition key:\n  #  [outputs.kinesis.partition]\n  #    method = \"measurement\"\n  #\n  ## Use the value of a tag for all writes, if the tag is not set the empty\n  ## default option will be used. When no default, defaults to \"telegraf\"\n  #  [outputs.kinesis.partition]\n  #    method = \"tag\"\n  #    key = \"host\"\n  #    default = \"mykey\"\n\n\n  ## Data format to output.\n  ## Each data format has its own unique set of configuration options, read\n  ## more about them here:\n  ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md\n  data_format = \"influx\"\n\n  ## debug will show upstream aws messages.\n  debug = false\n"
	},
	{
		"Name": "librato",
		"Description": " Configuration for Librato API to send metrics to.",
		"SampleConfig": "[[outputs.librato]]\n  ## Librato API Docs\n  ## http://dev.librato.com/v1/metrics-authentication\n  ## Librato API user\n  api_user = \"telegraf@influxdb.com\" # required.\n  ## Librato API token\n  api_token = \"my-secret-token\" # required.\n  ## Debug\n  # debug = false\n  ## Connection timeout.\n  # timeout = \"5s\"\n  ## Output source Template (same as graphite buckets)\n  ## see https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md#graphite\n  ## This template is used in librato's source (not metric's name)\n  template = \"host\"\n"
	},
	{
		"Name": "logzio",
		"Description": " A plugin that can send metrics over HTTPs to Logz.io",
		"SampleConfig": "[[outputs.logzio]]\n  ## Set to true if Logz.io sender checks the disk space before adding metrics to the disk queue.\n  # check_disk_space = true\n\n  ## The percent of used file system space at which the sender will stop queueing.\n  ## When we will reach that percentage, the file system in which the queue is stored will drop\n  ## all new logs until the percentage of used space drops below that threshold.\n  # disk_threshold = 98\n\n  ## How often Logz.io sender should drain the queue.\n  ## Valid time units are \"ns\", \"us\" (or \"Âµs\"), \"ms\", \"s\", \"m\", \"h\".\n  # drain_duration = \"3s\"\n\n  ## Where Logz.io sender should store the queue\n  ## queue_dir = Sprintf(\"%s%s%s%s%d\", os.TempDir(), string(os.PathSeparator),\n  ##                     \"logzio-buffer\", string(os.PathSeparator), time.Now().UnixNano())\n\n  ## Logz.io account token\n  token = \"your Logz.io token\" # required\n\n  ## Use your listener URL for your Logz.io account region.\n  # url = \"https://listener.logz.io:8071\"\n"
	},
	{
		"Name": "loki",
		"Description": " A plugin that can transmit logs to Loki",
		"SampleConfig": "[[outputs.loki]]\n  ## The domain of Loki\n  domain = \"https://loki.domain.tld\"\n\n  ## Endpoint to write api\n  # endpoint = \"/loki/api/v1/push\"\n\n  ## Connection timeout, defaults to \"5s\" if not set.\n  # timeout = \"5s\"\n\n  ## Basic auth credential\n  # username = \"loki\"\n  # password = \"pass\"\n\n  ## Additional HTTP headers\n  # http_headers = {\"X-Scope-OrgID\" = \"1\"}\n\n  ## If the request must be gzip encoded\n  # gzip_request = false\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n"
	},
	{
		"Name": "mongodb",
		"Description": " A plugin that can transmit logs to mongodb",
		"SampleConfig": "[[outputs.mongodb]]\n  # connection string examples for mongodb\n  dsn = \"mongodb://localhost:27017\"\n  # dsn = \"mongodb://mongod1:27017,mongod2:27017,mongod3:27017/admin\u0026replicaSet=myReplSet\u0026w=1\"\n\n  # overrides serverSelectionTimeoutMS in dsn if set\n  # timeout = \"30s\"\n\n  # default authentication, optional\n  # authentication = \"NONE\"\n\n  # for SCRAM-SHA-256 authentication\n  # authentication = \"SCRAM\"\n  # username = \"root\"\n  # password = \"***\"\n\n  # for x509 certificate authentication\n  # authentication = \"X509\"\n  # tls_ca = \"ca.pem\"\n  # tls_key = \"client.pem\"\n  # # tls_key_pwd = \"changeme\" # required for encrypted tls_key\n  # insecure_skip_verify = false\n\n  # database to store measurements and time series collections\n  # database = \"telegraf\"\n\n  # granularity can be seconds, minutes, or hours.\n  # configuring this value will be based on your input collection frequency.\n  # see https://docs.mongodb.com/manual/core/timeseries-collections/#create-a-time-series-collection\n  # granularity = \"seconds\"\n\n  # optionally set a TTL to automatically expire documents from the measurement collections.\n  # ttl = \"360h\"\n"
	},
	{
		"Name": "mqtt",
		"Description": " Configuration for MQTT server to send metrics to",
		"SampleConfig": "[[outputs.mqtt]]\n  ## MQTT Brokers\n  ## The list of brokers should only include the hostname or IP address and the\n  ## port to the broker. This should follow the format '{host}:{port}'. For\n  ## example, \"localhost:1883\" or \"127.0.0.1:8883\".\n  servers = [\"localhost:1883\"]\n\n  ## MQTT Topic for Producer Messages\n  ## MQTT outputs send metrics to this topic format:\n  ## \u003ctopic_prefix\u003e/\u003chostname\u003e/\u003cpluginname\u003e/ (e.g. prefix/web01.example.com/mem)\n  topic_prefix = \"telegraf\"\n\n  ## QoS policy for messages\n  ## The mqtt QoS policy for sending messages.\n  ## See https://www.ibm.com/support/knowledgecenter/en/SSFKSJ_9.0.0/com.ibm.mq.dev.doc/q029090_.htm\n  ##   0 = at most once\n  ##   1 = at least once\n  ##   2 = exactly once\n  # qos = 2\n\n  ## Keep Alive\n  ## Defines the maximum length of time that the broker and client may not\n  ## communicate. Defaults to 0 which turns the feature off.\n  ##\n  ## For version v2.0.12 and later mosquitto there is a bug\n  ## (see https://github.com/eclipse/mosquitto/issues/2117), which requires\n  ## this to be non-zero. As a reference eclipse/paho.mqtt.golang defaults to 30.\n  # keep_alive = 0\n\n  ## username and password to connect MQTT server.\n  # username = \"telegraf\"\n  # password = \"metricsmetricsmetricsmetrics\"\n\n  ## client ID\n  ## The unique client id to connect MQTT server. If this parameter is not set\n  ## then a random ID is generated.\n  # client_id = \"\"\n\n  ## Timeout for write operations. default: 5s\n  # timeout = \"5s\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n\n  ## When true, metrics will be sent in one MQTT message per flush. Otherwise,\n  ## metrics are written one metric per MQTT message.\n  # batch = false\n\n  ## When true, metric will have RETAIN flag set, making broker cache entries until someone\n  ## actually reads it\n  # retain = false\n\n  ## Each data format has its own unique set of configuration options, read\n  ## more about them here:\n  ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md\n  data_format = \"influx\"\n"
	},
	{
		"Name": "nats",
		"Description": " Send telegraf measurements to NATS",
		"SampleConfig": "[[outputs.nats]]\n  ## URLs of NATS servers\n  servers = [\"nats://localhost:4222\"]\n\n  ## Optional client name\n  # name = \"\"\n\n  ## Optional credentials\n  # username = \"\"\n  # password = \"\"\n\n  ## Optional NATS 2.0 and NATS NGS compatible user credentials\n  # credentials = \"/etc/telegraf/nats.creds\"\n\n  ## NATS subject for producer messages\n  subject = \"telegraf\"\n\n  ## Use Transport Layer Security\n  # secure = false\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n\n  ## Data format to output.\n  ## Each data format has its own unique set of configuration options, read\n  ## more about them here:\n  ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md\n  data_format = \"influx\"\n"
	},
	{
		"Name": "newrelic",
		"Description": " Send metrics to New Relic metrics endpoint",
		"SampleConfig": "[[outputs.newrelic]]\n  ## The 'insights_key' parameter requires a NR license key.\n  ## New Relic recommends you create one\n  ## with a convenient name such as TELEGRAF_INSERT_KEY.\n  ## reference: https://docs.newrelic.com/docs/apis/intro-apis/new-relic-api-keys/#ingest-license-key\n  # insights_key = \"New Relic License Key Here\"\n\n  ## Prefix to add to add to metric name for easy identification.\n  ## This is very useful if your metric names are ambiguous.\n  # metric_prefix = \"\"\n\n  ## Timeout for writes to the New Relic API.\n  # timeout = \"15s\"\n\n  ## HTTP Proxy override. If unset use values from the standard\n  ## proxy environment variables to determine proxy, if any.\n  # http_proxy = \"http://corporate.proxy:3128\"\n\n  ## Metric URL override to enable geographic location endpoints.\n  # If not set use values from the standard\n  # metric_url = \"https://metric-api.newrelic.com/metric/v1\"\n"
	},
	{
		"Name": "nsq",
		"Description": " Send telegraf measurements to NSQD",
		"SampleConfig": "[[outputs.nsq]]\n  ## Location of nsqd instance listening on TCP\n  server = \"localhost:4150\"\n  ## NSQ topic for producer messages\n  topic = \"telegraf\"\n\n  ## Data format to output.\n  ## Each data format has its own unique set of configuration options, read\n  ## more about them here:\n  ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md\n  data_format = \"influx\"\n"
	},
	{
		"Name": "opentelemetry",
		"Description": " Send OpenTelemetry metrics over gRPC",
		"SampleConfig": "[[outputs.opentelemetry]]\n  ## Override the default (localhost:4317) OpenTelemetry gRPC service\n  ## address:port\n  # service_address = \"localhost:4317\"\n\n  ## Override the default (5s) request timeout\n  # timeout = \"5s\"\n\n  ## Optional TLS Config.\n  ##\n  ## Root certificates for verifying server certificates encoded in PEM format.\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  ## The public and private keypairs for the client encoded in PEM format.\n  ## May contain intermediate certificates.\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS, but skip TLS chain and host verification.\n  # insecure_skip_verify = false\n  ## Send the specified TLS server name via SNI.\n  # tls_server_name = \"foo.example.com\"\n\n  ## Override the default (gzip) compression used to send data.\n  ## Supports: \"gzip\", \"none\"\n  # compression = \"gzip\"\n\n  ## Additional OpenTelemetry resource attributes\n  # [outputs.opentelemetry.attributes]\n  # \"service.name\" = \"demo\"\n\n  ## Additional gRPC request metadata\n  # [outputs.opentelemetry.headers]\n  # key1 = \"value1\"\n"
	},
	{
		"Name": "opentsdb",
		"Description": " Configuration for OpenTSDB server to send metrics to",
		"SampleConfig": "[[outputs.opentsdb]]\n  ## prefix for metrics keys\n  prefix = \"my.specific.prefix.\"\n\n  ## DNS name of the OpenTSDB server\n  ## Using \"opentsdb.example.com\" or \"tcp://opentsdb.example.com\" will use the\n  ## telnet API. \"http://opentsdb.example.com\" will use the Http API.\n  host = \"opentsdb.example.com\"\n\n  ## Port of the OpenTSDB server\n  port = 4242\n\n  ## Number of data points to send to OpenTSDB in Http requests.\n  ## Not used with telnet API.\n  http_batch_size = 50\n\n  ## URI Path for Http requests to OpenTSDB.\n  ## Used in cases where OpenTSDB is located behind a reverse proxy.\n  http_path = \"/api/put\"\n\n  ## Debug true - Prints OpenTSDB communication\n  debug = false\n\n  ## Separator separates measurement name from field\n  separator = \"_\"\n"
	},
	{
		"Name": "prometheus_client",
		"Description": " Configuration for the Prometheus client to spawn",
		"SampleConfig": "[[outputs.prometheus_client]]\n  ## Address to listen on.\n  listen = \":9273\"\n\n  ## Metric version controls the mapping from Prometheus metrics into Telegraf metrics.\n  ## See \"Metric Format Configuration\" in plugins/inputs/prometheus/README.md for details.\n  ## Valid options: 1, 2\n  # metric_version = 1\n\n  ## Use HTTP Basic Authentication.\n  # basic_username = \"Foo\"\n  # basic_password = \"Bar\"\n\n  ## If set, the IP Ranges which are allowed to access metrics.\n  ##   ex: ip_range = [\"192.168.0.0/24\", \"192.168.1.0/30\"]\n  # ip_range = []\n\n  ## Path to publish the metrics on.\n  # path = \"/metrics\"\n\n  ## Expiration interval for each metric. 0 == no expiration\n  # expiration_interval = \"60s\"\n\n  ## Collectors to enable, valid entries are \"gocollector\" and \"process\".\n  ## If unset, both are enabled.\n  # collectors_exclude = [\"gocollector\", \"process\"]\n\n  ## Send string metrics as Prometheus labels.\n  ## Unless set to false all string metrics will be sent as labels.\n  # string_as_label = true\n\n  ## If set, enable TLS with the given certificate.\n  # tls_cert = \"/etc/ssl/telegraf.crt\"\n  # tls_key = \"/etc/ssl/telegraf.key\"\n\n  ## Set one or more allowed client CA certificate file names to\n  ## enable mutually authenticated TLS connections\n  # tls_allowed_cacerts = [\"/etc/telegraf/clientca.pem\"]\n\n  ## Export metric collection time.\n  # export_timestamp = false\n"
	},
	{
		"Name": "riemann",
		"Description": " Configuration for Riemann to send metrics to",
		"SampleConfig": "[[outputs.riemann]]\n  ## The full TCP or UDP URL of the Riemann server\n  url = \"tcp://localhost:5555\"\n\n  ## Riemann event TTL, floating-point time in seconds.\n  ## Defines how long that an event is considered valid for in Riemann\n  # ttl = 30.0\n\n  ## Separator to use between measurement and field name in Riemann service name\n  ## This does not have any effect if 'measurement_as_attribute' is set to 'true'\n  separator = \"/\"\n\n  ## Set measurement name as Riemann attribute 'measurement', instead of prepending it to the Riemann service name\n  # measurement_as_attribute = false\n\n  ## Send string metrics as Riemann event states.\n  ## Unless enabled all string metrics will be ignored\n  # string_as_state = false\n\n  ## A list of tag keys whose values get sent as Riemann tags.\n  ## If empty, all Telegraf tag values will be sent as tags\n  # tag_keys = [\"telegraf\",\"custom_tag\"]\n\n  ## Additional Riemann tags to send.\n  # tags = [\"telegraf-output\"]\n\n  ## Description for Riemann event\n  # description_text = \"metrics collected from telegraf\"\n\n  ## Riemann client write timeout, defaults to \"5s\" if not set.\n  # timeout = \"5s\"\n"
	},
	{
		"Name": "riemann_legacy",
		"Description": " Configuration for the Riemann server to send metrics to",
		"SampleConfig": "[[outputs.riemann_legacy]]\n  ## URL of server\n  url = \"localhost:5555\"\n  ## transport protocol to use either tcp or udp\n  transport = \"tcp\"\n  ## separator to use between input name and field name in Riemann service name\n  separator = \" \"\n"
	},
	{
		"Name": "sensu",
		"Description": " Send aggregate metrics to Sensu Monitor",
		"SampleConfig": "[[outputs.sensu]]\n  ## BACKEND API URL is the Sensu Backend API root URL to send metrics to\n  ## (protocol, host, and port only). The output plugin will automatically\n  ## append the corresponding backend API path\n  ## /api/core/v2/namespaces/:entity_namespace/events/:entity_name/:check_name).\n  ##\n  ## Backend Events API reference:\n  ## https://docs.sensu.io/sensu-go/latest/api/events/\n  ##\n  ## AGENT API URL is the Sensu Agent API root URL to send metrics to\n  ## (protocol, host, and port only). The output plugin will automatically\n  ## append the correspeonding agent API path (/events).\n  ##\n  ## Agent API Events API reference:\n  ## https://docs.sensu.io/sensu-go/latest/api/events/\n  ##\n  ## NOTE: if backend_api_url and agent_api_url and api_key are set, the output\n  ## plugin will use backend_api_url. If backend_api_url and agent_api_url are\n  ## not provided, the output plugin will default to use an agent_api_url of\n  ## http://127.0.0.1:3031\n  ##\n  # backend_api_url = \"http://127.0.0.1:8080\"\n  # agent_api_url = \"http://127.0.0.1:3031\"\n\n  ## API KEY is the Sensu Backend API token\n  ## Generate a new API token via:\n  ##\n  ## $ sensuctl cluster-role create telegraf --verb create --resource events,entities\n  ## $ sensuctl cluster-role-binding create telegraf --cluster-role telegraf --group telegraf\n  ## $ sensuctl user create telegraf --group telegraf --password REDACTED\n  ## $ sensuctl api-key grant telegraf\n  ##\n  ## For more information on Sensu RBAC profiles \u0026 API tokens, please visit:\n  ## - https://docs.sensu.io/sensu-go/latest/reference/rbac/\n  ## - https://docs.sensu.io/sensu-go/latest/reference/apikeys/\n  ##\n  # api_key = \"${SENSU_API_KEY}\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n\n  ## Timeout for HTTP message\n  # timeout = \"5s\"\n\n  ## HTTP Content-Encoding for write request body, can be set to \"gzip\" to\n  ## compress body or \"identity\" to apply no encoding.\n  # content_encoding = \"identity\"\n\n  ## Sensu Event details\n  ##\n  ## Below are the event details to be sent to Sensu.  The main portions of the\n  ## event are the check, entity, and metrics specifications. For more information\n  ## on Sensu events and its components, please visit:\n  ## - Events - https://docs.sensu.io/sensu-go/latest/reference/events\n  ## - Checks -  https://docs.sensu.io/sensu-go/latest/reference/checks\n  ## - Entities - https://docs.sensu.io/sensu-go/latest/reference/entities\n  ## - Metrics - https://docs.sensu.io/sensu-go/latest/reference/events#metrics\n  ##\n  ## Check specification\n  ## The check name is the name to give the Sensu check associated with the event\n  ## created. This maps to check.metatadata.name in the event.\n  [outputs.sensu.check]\n    name = \"telegraf\"\n\n  ## Entity specification\n  ## Configure the entity name and namespace, if necessary. This will be part of\n  ## the entity.metadata in the event.\n  ##\n  ## NOTE: if the output plugin is configured to send events to a\n  ## backend_api_url and entity_name is not set, the value returned by\n  ## os.Hostname() will be used; if the output plugin is configured to send\n  ## events to an agent_api_url, entity_name and entity_namespace are not used.\n  # [outputs.sensu.entity]\n  #   name = \"server-01\"\n  #   namespace = \"default\"\n\n  ## Metrics specification\n  ## Configure the tags for the metrics that are sent as part of the Sensu event\n  # [outputs.sensu.tags]\n  #   source = \"telegraf\"\n\n  ## Configure the handler(s) for processing the provided metrics\n  # [outputs.sensu.metrics]\n  #   handlers = [\"influxdb\",\"elasticsearch\"]\n"
	},
	{
		"Name": "signalfx",
		"Description": " Send metrics and events to SignalFx",
		"SampleConfig": "[[outputs.signalfx]]\n  ## SignalFx Org Access Token\n  access_token = \"my-secret-token\"\n\n  ## The SignalFx realm that your organization resides in\n  signalfx_realm = \"us9\"  # Required if ingest_url is not set\n\n  ## You can optionally provide a custom ingest url instead of the\n  ## signalfx_realm option above if you are using a gateway or proxy\n  ## instance.  This option takes precident over signalfx_realm.\n  ingest_url = \"https://my-custom-ingest/\"\n\n  ## Event typed metrics are omitted by default,\n  ## If you require an event typed metric you must specify the\n  ## metric name in the following list.\n  included_event_names = [\"plugin.metric_name\"]\n"
	},
	{
		"Name": "socket_writer",
		"Description": " Generic socket writer capable of handling multiple socket types.",
		"SampleConfig": "[[outputs.socket_writer]]\n  ## URL to connect to\n  # address = \"tcp://127.0.0.1:8094\"\n  # address = \"tcp://example.com:http\"\n  # address = \"tcp4://127.0.0.1:8094\"\n  # address = \"tcp6://127.0.0.1:8094\"\n  # address = \"tcp6://[2001:db8::1]:8094\"\n  # address = \"udp://127.0.0.1:8094\"\n  # address = \"udp4://127.0.0.1:8094\"\n  # address = \"udp6://127.0.0.1:8094\"\n  # address = \"unix:///tmp/telegraf.sock\"\n  # address = \"unixgram:///tmp/telegraf.sock\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n\n  ## Period between keep alive probes.\n  ## Only applies to TCP sockets.\n  ## 0 disables keep alive probes.\n  ## Defaults to the OS configuration.\n  # keep_alive_period = \"5m\"\n\n  ## Content encoding for message payloads, can be set to \"gzip\" or to\n  ## \"identity\" to apply no encoding.\n  ##\n  # content_encoding = \"identity\"\n\n  ## Data format to generate.\n  ## Each data format has its own unique set of configuration options, read\n  ## more about them here:\n  ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md\n  # data_format = \"influx\"\n"
	},
	{
		"Name": "sql",
		"Description": " Save metrics to an SQL Database",
		"SampleConfig": "[[outputs.sql]]\n  ## Database driver\n  ## Valid options: mssql (Microsoft SQL Server), mysql (MySQL), pgx (Postgres),\n  ##  sqlite (SQLite3), snowflake (snowflake.com) clickhouse (ClickHouse)\n  # driver = \"\"\n\n  ## Data source name\n  ## The format of the data source name is different for each database driver.\n  ## See the plugin readme for details.\n  # data_source_name = \"\"\n\n  ## Timestamp column name\n  # timestamp_column = \"timestamp\"\n\n  ## Table creation template\n  ## Available template variables:\n  ##  {TABLE} - table name as a quoted identifier\n  ##  {TABLELITERAL} - table name as a quoted string literal\n  ##  {COLUMNS} - column definitions (list of quoted identifiers and types)\n  # table_template = \"CREATE TABLE {TABLE}({COLUMNS})\"\n\n  ## Table existence check template\n  ## Available template variables:\n  ##  {TABLE} - tablename as a quoted identifier\n  # table_exists_template = \"SELECT 1 FROM {TABLE} LIMIT 1\"\n\n  ## Initialization SQL\n  # init_sql = \"\"\n\n  ## Metric type to SQL type conversion\n  ## The values on the left are the data types Telegraf has and the values on\n  ## the right are the data types Telegraf will use when sending to a database.\n  ##\n  ## The database values used must be data types the destination database\n  ## understands. It is up to the user to ensure that the selected data type is\n  ## available in the database they are using. Refer to your database\n  ## documentation for what data types are available and supported.\n  #[outputs.sql.convert]\n  #  integer              = \"INT\"\n  #  real                 = \"DOUBLE\"\n  #  text                 = \"TEXT\"\n  #  timestamp            = \"TIMESTAMP\"\n  #  defaultvalue         = \"TEXT\"\n  #  unsigned             = \"UNSIGNED\"\n  #  bool                 = \"BOOL\"\n\n  ## This setting controls the behavior of the unsigned value. By default the\n  ## setting will take the integer value and append the unsigned value to it. The other\n  ## option is \"literal\", which will use the actual value the user provides to\n  ## the unsigned option. This is useful for a database like ClickHouse where\n  ## the unsigned value should use a value like \"uint64\".\n  # conversion_style = \"unsigned_suffix\"\n"
	},
	{
		"Name": "stackdriver",
		"Description": " Configuration for Google Cloud Stackdriver to send metrics to",
		"SampleConfig": "[[outputs.stackdriver]]\n  ## GCP Project\n  project = \"erudite-bloom-151019\"\n\n  ## The namespace for the metric descriptor\n  namespace = \"telegraf\"\n\n  ## Custom resource type\n  # resource_type = \"generic_node\"\n\n  ## Additional resource labels\n  # [outputs.stackdriver.resource_labels]\n  #   node_id = \"$HOSTNAME\"\n  #   namespace = \"myapp\"\n  #   location = \"eu-north0\"\n"
	},
	{
		"Name": "sumologic",
		"Description": " A plugin that can send metrics to Sumo Logic HTTP metric collector.",
		"SampleConfig": "[[outputs.sumologic]]\n  ## Unique URL generated for your HTTP Metrics Source.\n  ## This is the address to send metrics to.\n  # url = \"https://events.sumologic.net/receiver/v1/http/\u003cUniqueHTTPCollectorCode\u003e\"\n\n  ## Data format to be used for sending metrics.\n  ## This will set the \"Content-Type\" header accordingly.\n  ## Currently supported formats:\n  ## * graphite - for Content-Type of application/vnd.sumologic.graphite\n  ## * carbon2 - for Content-Type of application/vnd.sumologic.carbon2\n  ## * prometheus - for Content-Type of application/vnd.sumologic.prometheus\n  ##\n  ## More information can be found at:\n  ## https://help.sumologic.com/03Send-Data/Sources/02Sources-for-Hosted-Collectors/HTTP-Source/Upload-Metrics-to-an-HTTP-Source#content-type-headers-for-metrics\n  ##\n  ## NOTE:\n  ## When unset, telegraf will by default use the influx serializer which is currently unsupported\n  ## in HTTP Source.\n  data_format = \"carbon2\"\n\n  ## Timeout used for HTTP request\n  # timeout = \"5s\"\n\n  ## Max HTTP request body size in bytes before compression (if applied).\n  ## By default 1MB is recommended.\n  ## NOTE:\n  ## Bear in mind that in some serializer a metric even though serialized to multiple\n  ## lines cannot be split any further so setting this very low might not work\n  ## as expected.\n  # max_request_body_size = 1000000\n\n  ## Additional, Sumo specific options.\n  ## Full list can be found here:\n  ## https://help.sumologic.com/03Send-Data/Sources/02Sources-for-Hosted-Collectors/HTTP-Source/Upload-Metrics-to-an-HTTP-Source#supported-http-headers\n\n  ## Desired source name.\n  ## Useful if you want to override the source name configured for the source.\n  # source_name = \"\"\n\n  ## Desired host name.\n  ## Useful if you want to override the source host configured for the source.\n  # source_host = \"\"\n\n  ## Desired source category.\n  ## Useful if you want to override the source category configured for the source.\n  # source_category = \"\"\n\n  ## Comma-separated key=value list of dimensions to apply to every metric.\n  ## Custom dimensions will allow you to query your metrics at a more granular level.\n  # dimensions = \"\"\n"
	},
	{
		"Name": "syslog",
		"Description": " Configuration for Syslog server to send metrics to",
		"SampleConfig": "[[outputs.syslog]]\n  ## URL to connect to\n  ## ex: address = \"tcp://127.0.0.1:8094\"\n  ## ex: address = \"tcp4://127.0.0.1:8094\"\n  ## ex: address = \"tcp6://127.0.0.1:8094\"\n  ## ex: address = \"tcp6://[2001:db8::1]:8094\"\n  ## ex: address = \"udp://127.0.0.1:8094\"\n  ## ex: address = \"udp4://127.0.0.1:8094\"\n  ## ex: address = \"udp6://127.0.0.1:8094\"\n  address = \"tcp://127.0.0.1:8094\"\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n\n  ## Period between keep alive probes.\n  ## Only applies to TCP sockets.\n  ## 0 disables keep alive probes.\n  ## Defaults to the OS configuration.\n  # keep_alive_period = \"5m\"\n\n  ## The framing technique with which it is expected that messages are\n  ## transported (default = \"octet-counting\").  Whether the messages come\n  ## using the octect-counting (RFC5425#section-4.3.1, RFC6587#section-3.4.1),\n  ## or the non-transparent framing technique (RFC6587#section-3.4.2).  Must\n  ## be one of \"octet-counting\", \"non-transparent\".\n  # framing = \"octet-counting\"\n\n  ## The trailer to be expected in case of non-transparent framing (default = \"LF\").\n  ## Must be one of \"LF\", or \"NUL\".\n  # trailer = \"LF\"\n\n  ## SD-PARAMs settings\n  ## Syslog messages can contain key/value pairs within zero or more\n  ## structured data sections.  For each unrecognized metric tag/field a\n  ## SD-PARAMS is created.\n  ##\n  ## Example:\n  ##   [[outputs.syslog]]\n  ##     sdparam_separator = \"_\"\n  ##     default_sdid = \"default@32473\"\n  ##     sdids = [\"foo@123\", \"bar@456\"]\n  ##\n  ##   input =\u003e xyzzy,x=y foo@123_value=42,bar@456_value2=84,something_else=1\n  ##   output (structured data only) =\u003e [foo@123 value=42][bar@456 value2=84][default@32473 something_else=1 x=y]\n\n  ## SD-PARAMs separator between the sdid and tag/field key (default = \"_\")\n  # sdparam_separator = \"_\"\n\n  ## Default sdid used for tags/fields that don't contain a prefix defined in\n  ## the explicit sdids setting below If no default is specified, no SD-PARAMs\n  ## will be used for unrecognized field.\n  # default_sdid = \"default@32473\"\n\n  ## List of explicit prefixes to extract from tag/field keys and use as the\n  ## SDID, if they match (see above example for more details):\n  # sdids = [\"foo@123\", \"bar@456\"]\n\n  ## Default severity value. Severity and Facility are used to calculate the\n  ## message PRI value (RFC5424#section-6.2.1).  Used when no metric field\n  ## with key \"severity_code\" is defined.  If unset, 5 (notice) is the default\n  # default_severity_code = 5\n\n  ## Default facility value. Facility and Severity are used to calculate the\n  ## message PRI value (RFC5424#section-6.2.1).  Used when no metric field with\n  ## key \"facility_code\" is defined.  If unset, 1 (user-level) is the default\n  # default_facility_code = 1\n\n  ## Default APP-NAME value (RFC5424#section-6.2.5)\n  ## Used when no metric tag with key \"appname\" is defined.\n  ## If unset, \"Telegraf\" is the default\n  # default_appname = \"Telegraf\"\n"
	},
	{
		"Name": "timestream",
		"Description": " Configuration for sending metrics to Amazon Timestream.",
		"SampleConfig": "[[outputs.timestream]]\n  ## Amazon Region\n  region = \"us-east-1\"\n\n  ## Amazon Credentials\n  ## Credentials are loaded in the following order\n  ## 1) Web identity provider credentials via STS if role_arn and web_identity_token_file are specified\n  ## 2) Assumed credentials via STS if role_arn is specified\n  ## 3) explicit credentials from 'access_key' and 'secret_key'\n  ## 4) shared profile from 'profile'\n  ## 5) environment variables\n  ## 6) shared credentials file\n  ## 7) EC2 Instance Profile\n  #access_key = \"\"\n  #secret_key = \"\"\n  #token = \"\"\n  #role_arn = \"\"\n  #web_identity_token_file = \"\"\n  #role_session_name = \"\"\n  #profile = \"\"\n  #shared_credential_file = \"\"\n\n  ## Endpoint to make request against, the correct endpoint is automatically\n  ## determined and this option should only be set if you wish to override the\n  ## default.\n  ##   ex: endpoint_url = \"http://localhost:8000\"\n  # endpoint_url = \"\"\n\n  ## Timestream database where the metrics will be inserted.\n  ## The database must exist prior to starting Telegraf.\n  database_name = \"yourDatabaseNameHere\"\n\n  ## Specifies if the plugin should describe the Timestream database upon starting\n  ## to validate if it has access necessary permissions, connection, etc., as a safety check.\n  ## If the describe operation fails, the plugin will not start\n  ## and therefore the Telegraf agent will not start.\n  describe_database_on_start = false\n\n  ## The mapping mode specifies how Telegraf records are represented in Timestream.\n  ## Valid values are: single-table, multi-table.\n  ## For example, consider the following data in line protocol format:\n  ## weather,location=us-midwest,season=summer temperature=82,humidity=71 1465839830100400200\n  ## airquality,location=us-west no2=5,pm25=16 1465839830100400200\n  ## where weather and airquality are the measurement names, location and season are tags,\n  ## and temperature, humidity, no2, pm25 are fields.\n  ## In multi-table mode:\n  ##  - first line will be ingested to table named weather\n  ##  - second line will be ingested to table named airquality\n  ##  - the tags will be represented as dimensions\n  ##  - first table (weather) will have two records:\n  ##      one with measurement name equals to temperature,\n  ##      another with measurement name equals to humidity\n  ##  - second table (airquality) will have two records:\n  ##      one with measurement name equals to no2,\n  ##      another with measurement name equals to pm25\n  ##  - the Timestream tables from the example will look like this:\n  ##      TABLE \"weather\":\n  ##        time | location | season | measure_name | measure_value::bigint\n  ##        2016-06-13 17:43:50 | us-midwest | summer | temperature | 82\n  ##        2016-06-13 17:43:50 | us-midwest | summer | humidity | 71\n  ##      TABLE \"airquality\":\n  ##        time | location | measure_name | measure_value::bigint\n  ##        2016-06-13 17:43:50 | us-west | no2 | 5\n  ##        2016-06-13 17:43:50 | us-west | pm25 | 16\n  ## In single-table mode:\n  ##  - the data will be ingested to a single table, which name will be valueOf(single_table_name)\n  ##  - measurement name will stored in dimension named valueOf(single_table_dimension_name_for_telegraf_measurement_name)\n  ##  - location and season will be represented as dimensions\n  ##  - temperature, humidity, no2, pm25 will be represented as measurement name\n  ##  - the Timestream table from the example will look like this:\n  ##      Assuming:\n  ##        - single_table_name = \"my_readings\"\n  ##        - single_table_dimension_name_for_telegraf_measurement_name = \"namespace\"\n  ##      TABLE \"my_readings\":\n  ##        time | location | season | namespace | measure_name | measure_value::bigint\n  ##        2016-06-13 17:43:50 | us-midwest | summer | weather | temperature | 82\n  ##        2016-06-13 17:43:50 | us-midwest | summer | weather | humidity | 71\n  ##        2016-06-13 17:43:50 | us-west | NULL | airquality | no2 | 5\n  ##        2016-06-13 17:43:50 | us-west | NULL | airquality | pm25 | 16\n  ## In most cases, using multi-table mapping mode is recommended.\n  ## However, you can consider using single-table in situations when you have thousands of measurement names.\n  mapping_mode = \"multi-table\"\n\n  ## Only valid and required for mapping_mode = \"single-table\"\n  ## Specifies the Timestream table where the metrics will be uploaded.\n  # single_table_name = \"yourTableNameHere\"\n\n  ## Only valid and required for mapping_mode = \"single-table\"\n  ## Describes what will be the Timestream dimension name for the Telegraf\n  ## measurement name.\n  # single_table_dimension_name_for_telegraf_measurement_name = \"namespace\"\n\n  ## Specifies if the plugin should create the table, if the table do not exist.\n  ## The plugin writes the data without prior checking if the table exists.\n  ## When the table does not exist, the error returned from Timestream will cause\n  ## the plugin to create the table, if this parameter is set to true.\n  create_table_if_not_exists = true\n\n  ## Only valid and required if create_table_if_not_exists = true\n  ## Specifies the Timestream table magnetic store retention period in days.\n  ## Check Timestream documentation for more details.\n  create_table_magnetic_store_retention_period_in_days = 365\n\n  ## Only valid and required if create_table_if_not_exists = true\n  ## Specifies the Timestream table memory store retention period in hours.\n  ## Check Timestream documentation for more details.\n  create_table_memory_store_retention_period_in_hours = 24\n\n  ## Only valid and optional if create_table_if_not_exists = true\n  ## Specifies the Timestream table tags.\n  ## Check Timestream documentation for more details\n  # create_table_tags = { \"foo\" = \"bar\", \"environment\" = \"dev\"}\n\n  ## Specify the maximum number of parallel go routines to ingest/write data\n  ## If not specified, defaulted to 1 go routines\n  max_write_go_routines = 25\n"
	},
	{
		"Name": "warp10",
		"Description": " Write metrics to Warp 10",
		"SampleConfig": "[[outputs.warp10]]\n  # Prefix to add to the measurement.\n  prefix = \"telegraf.\"\n\n  # URL of the Warp 10 server\n  warp_url = \"http://localhost:8080\"\n\n  # Write token to access your app on warp 10\n  token = \"Token\"\n\n  # Warp 10 query timeout\n  # timeout = \"15s\"\n\n  ## Print Warp 10 error body\n  # print_error_body = false\n\n  ##Â Max string error size\n  # max_string_error_size = 511\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n"
	},
	{
		"Name": "wavefront",
		"Description": " Configuration for Wavefront server to send metrics to",
		"SampleConfig": "[[outputs.wavefront]]\n  ## Url for Wavefront Direct Ingestion. For Wavefront Proxy Ingestion, see\n  ## the 'host' and 'port' options below.\n  url = \"https://metrics.wavefront.com\"\n\n  ## Authentication Token for Wavefront. Only required if using Direct Ingestion\n  #token = \"DUMMY_TOKEN\"\n\n  ## DNS name of the wavefront proxy server. Do not use if url is specified\n  #host = \"wavefront.example.com\"\n\n  ## Port that the Wavefront proxy server listens on. Do not use if url is specified\n  #port = 2878\n\n  ## prefix for metrics keys\n  #prefix = \"my.specific.prefix.\"\n\n  ## whether to use \"value\" for name of simple fields. default is false\n  #simple_fields = false\n\n  ## character to use between metric and field name.  default is . (dot)\n  #metric_separator = \".\"\n\n  ## Convert metric name paths to use metricSeparator character\n  ## When true will convert all _ (underscore) characters in final metric name. default is true\n  #convert_paths = true\n\n  ## Use Strict rules to sanitize metric and tag names from invalid characters\n  ## When enabled forward slash (/) and comma (,) will be accepted\n  #use_strict = false\n\n  ## Use Regex to sanitize metric and tag names from invalid characters\n  ## Regex is more thorough, but significantly slower. default is false\n  #use_regex = false\n\n  ## point tags to use as the source name for Wavefront (if none found, host will be used)\n  #source_override = [\"hostname\", \"address\", \"agent_host\", \"node_host\"]\n\n  ## whether to convert boolean values to numeric values, with false -\u003e 0.0 and true -\u003e 1.0. default is true\n  #convert_bool = true\n\n  ## Truncate metric tags to a total of 254 characters for the tag name value. Wavefront will reject any\n  ## data point exceeding this limit if not truncated. Defaults to 'false' to provide backwards compatibility.\n  #truncate_tags = false\n\n  ## Flush the internal buffers after each batch. This effectively bypasses the background sending of metrics\n  ## normally done by the Wavefront SDK. This can be used if you are experiencing buffer overruns. The sending\n  ## of metrics will block for a longer time, but this will be handled gracefully by the internal buffering in\n  ## Telegraf.\n  #immediate_flush = true\n"
	},
	{
		"Name": "websocket",
		"Description": " A plugin that can transmit metrics over WebSocket.",
		"SampleConfig": "[[outputs.websocket]]\n  ## URL is the address to send metrics to. Make sure ws or wss scheme is used.\n  url = \"ws://127.0.0.1:3000/telegraf\"\n\n  ## Timeouts (make sure read_timeout is larger than server ping interval or set to zero).\n  # connect_timeout = \"30s\"\n  # write_timeout = \"30s\"\n  # read_timeout = \"30s\"\n\n  ## Optionally turn on using text data frames (binary by default).\n  # use_text_frames = false\n\n  ## Optional TLS Config\n  # tls_ca = \"/etc/telegraf/ca.pem\"\n  # tls_cert = \"/etc/telegraf/cert.pem\"\n  # tls_key = \"/etc/telegraf/key.pem\"\n  ## Use TLS but skip chain \u0026 host verification\n  # insecure_skip_verify = false\n\n  ## Optional SOCKS5 proxy to use\n  # socks5_enabled = true\n  # socks5_address = \"127.0.0.1:1080\"\n  # socks5_username = \"alice\"\n  # socks5_password = \"pass123\"\n\n  ## Data format to output.\n  ## Each data format has it's own unique set of configuration options, read\n  ## more about them here:\n  ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md\n  # data_format = \"influx\"\n\n  ## Additional HTTP Upgrade headers\n  # [outputs.websocket.headers]\n  #   Authorization = \"Bearer \u003cTOKEN\u003e\"\n"
	},
	{
		"Name": "yandex_cloud_monitoring",
		"Description": " Send aggregated metrics to Yandex.Cloud Monitoring",
		"SampleConfig": "[[outputs.yandex_cloud_monitoring]]\n  ## Timeout for HTTP writes.\n  # timeout = \"20s\"\n\n  ## Yandex.Cloud monitoring API endpoint. Normally should not be changed\n  # endpoint_url = \"https://monitoring.api.cloud.yandex.net/monitoring/v2/data/write\"\n\n  ## All user metrics should be sent with \"custom\" service specified. Normally should not be changed\n  # service = \"custom\"\n"
	}
]